version: "3"

services:

  # hadoop master

  hadoopmaster:
    image: alako/hadoop
    hostname: hadoopmaster
    container_name: hadoopmaster
    restart: unless-stopped
    ports:
      - "9870:9870" # web monitor
      - "9000:9000" # master slot
      - "9868:9868" # secondary namenode
    volumes:
      - ./files/hdfs/master:/var/opt/hdfs
    command:
      - /opt/init-master.sh

  # spark master

  sparkmaster:
    image: alako/sparkmaster
    hostname: sparkmaster
    container_name: sparkmaster
    restart: unless-stopped
    ports:
      - "8080:8080" # web monitor
      - "7077:7077" # master slot

  # vectorize_api

  vectorize_api:
    image: alako/vectorize_api
    container_name: vectorize_api
    restart: unless-stopped
    ports:
      - "5050:5050"

  # scala api

  #scala_api:
    #image: alako/vectorize_api
    #container_name: vectorize_api
    #restart: unless-stopped
    #depends_on:
    # - "vectorize_api"
    #ports:
      #- "5050:5050"

  # hadoop worker

  hadoopworker:
    image: alako/hadoop
    container_name: hadoopworker
    restart: unless-stopped
    ports:
      - "9867:9867" # metadata operations
      - "9866:9866" # data transfer
      - "9865:9865" # web monitor
      - "9864:9864" # web monitor
    volumes:
      - ./files/hdfs/worker:/var/opt/hdfs
    command:
      - /opt/init-worker.sh

  # spark worker

  sparkworker:
    image: alako/sparkworker
    container_name: sparkworker
    restart: unless-stopped
    ports:
      - "8581:8581" # spark worker
    environment:
      - SPARK_WORKER_PORT=8581

